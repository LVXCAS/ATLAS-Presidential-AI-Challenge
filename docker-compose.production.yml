version: '3.8'

services:
  # Nginx reverse proxy and load balancer
  nginx:
    build:
      context: .
      dockerfile: docker/Dockerfile.nginx
    container_name: hive-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./docker/nginx/ssl:/etc/nginx/ssl
      - static_files:/usr/share/nginx/html/static
    networks:
      - hive_network
      - hive_monitoring
    restart: unless-stopped
    depends_on:
      - frontend
      - backend
      - grafana

  # Frontend React application
  frontend:
    build:
      context: ./frontend
      dockerfile: ../docker/Dockerfile.frontend
      target: production
    container_name: hive-frontend
    environment:
      - NODE_ENV=production
      - REACT_APP_API_URL=https://localhost/api
      - REACT_APP_WS_URL=wss://localhost/ws
      - GENERATE_SOURCEMAP=false
    volumes:
      - static_files:/app/build
    networks:
      - hive_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API service
  backend:
    build:
      context: ./backend
      dockerfile: ../docker/Dockerfile.backend
      target: production
    container_name: hive-backend
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - API_PORT=8001
      - DATABASE_URL=postgresql://hive_user:${DB_PASSWORD}@timescaledb:5432/hive_trading
      - REDIS_URL=redis://redis:6379/0
      - ALPACA_API_KEY=${ALPACA_API_KEY}
      - ALPACA_SECRET_KEY=${ALPACA_SECRET_KEY}
      - ALPACA_BASE_URL=${ALPACA_BASE_URL:-https://paper-api.alpaca.markets}
      - JWT_SECRET=${JWT_SECRET}
      - ALLOWED_ORIGINS=https://localhost,https://your-domain.com
      - LOG_LEVEL=warning
      - SENTRY_DSN=${SENTRY_DSN}
    volumes:
      - ./logs:/app/logs
      - ./data/models:/app/data/models
      - ./config:/app/config
    networks:
      - hive_network
      - hive_monitoring
    restart: unless-stopped
    depends_on:
      timescaledb:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # TimescaleDB database
  timescaledb:
    image: timescale/timescaledb-ha:pg14-latest
    container_name: hive-timescaledb
    environment:
      - POSTGRES_DB=hive_trading
      - POSTGRES_USER=hive_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
      - TIMESCALEDB_TELEMETRY=off
      - TS_TUNE_MEMORY=${DB_MEMORY:-4GB}
      - TS_TUNE_NUM_CPUS=${DB_CPUS:-2}
    volumes:
      - timescale_data:/home/postgres/pgdata/data
      - ./database/init:/docker-entrypoint-initdb.d
      - ./database/backups:/backups
    ports:
      - "127.0.0.1:5432:5432"
    networks:
      - hive_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive_user -d hive_trading"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: |
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200

  # Redis cache and message broker
  redis:
    image: redis:7-alpine
    container_name: hive-redis
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
      - ./docker/redis/redis.conf:/etc/redis/redis.conf
    ports:
      - "127.0.0.1:6379:6379"
    networks:
      - hive_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    command: redis-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD}

  # Prometheus metrics collection
  prometheus:
    image: prom/prometheus:v2.40.0
    container_name: hive-prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus_data:/prometheus
    ports:
      - "127.0.0.1:9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=90d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--storage.tsdb.wal-compression'
    networks:
      - hive_monitoring
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana dashboards
  grafana:
    image: grafana/grafana:9.3.2
    container_name: hive-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=redis-datasource,grafana-worldmap-panel,grafana-piechart-panel
      - GF_SERVER_ROOT_URL=https://localhost/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "127.0.0.1:3001:3000"
    networks:
      - hive_monitoring
    restart: unless-stopped
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AlertManager for notifications
  alertmanager:
    image: prom/alertmanager:v0.25.0
    container_name: hive-alertmanager
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    ports:
      - "127.0.0.1:9093:9093"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://localhost/alertmanager/'
      - '--cluster.advertise-address=0.0.0.0:9093'
    networks:
      - hive_monitoring
    restart: unless-stopped

  # Jaeger distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.39
    container_name: hive-jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - JAEGER_DISABLED=false
    ports:
      - "127.0.0.1:16686:16686"  # Jaeger UI
      - "127.0.0.1:14268:14268"  # Jaeger collector HTTP
    networks:
      - hive_monitoring
    restart: unless-stopped

  # Node Exporter for system metrics
  node_exporter:
    image: prom/node-exporter:v1.4.0
    container_name: hive-node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "127.0.0.1:9100:9100"
    networks:
      - hive_monitoring
    restart: unless-stopped

  # Redis Exporter for Redis metrics
  redis_exporter:
    image: oliver006/redis_exporter:v1.45.0
    container_name: hive-redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    ports:
      - "127.0.0.1:9121:9121"
    networks:
      - hive_monitoring
      - hive_network
    restart: unless-stopped
    depends_on:
      - redis

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.46.0
    container_name: hive-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "127.0.0.1:8080:8080"
    networks:
      - hive_monitoring
    restart: unless-stopped
    privileged: true
    devices:
      - /dev/kmsg

  # Loki for log aggregation
  loki:
    image: grafana/loki:2.7.1
    container_name: hive-loki
    volumes:
      - ./monitoring/loki/loki.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    ports:
      - "127.0.0.1:3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - hive_monitoring
    restart: unless-stopped

  # Promtail for log collection
  promtail:
    image: grafana/promtail:2.7.1
    container_name: hive-promtail
    volumes:
      - ./monitoring/promtail/promtail.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./logs:/app/logs:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - hive_monitoring
    restart: unless-stopped
    depends_on:
      - loki

  # Backup service
  backup:
    build:
      context: .
      dockerfile: docker/Dockerfile.backup
    container_name: hive-backup
    environment:
      - DB_HOST=timescaledb
      - DB_NAME=hive_trading
      - DB_USER=hive_user
      - DB_PASSWORD=${DB_PASSWORD}
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}  # Daily at 2 AM
      - RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-30}
      - S3_BUCKET=${S3_BACKUP_BUCKET}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./database/backups:/backups
      - ./data:/app/data:ro
    networks:
      - hive_network
    restart: unless-stopped
    depends_on:
      - timescaledb

volumes:
  timescale_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local
  loki_data:
    driver: local
  static_files:
    driver: local

networks:
  hive_network:
    driver: bridge
    name: hive_network
    ipam:
      config:
        - subnet: 172.20.0.0/16
  hive_monitoring:
    driver: bridge
    name: hive_monitoring
    ipam:
      config:
        - subnet: 172.21.0.0/16