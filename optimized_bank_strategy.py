"""
OPTIMIZED BANK TRADING STRATEGY
===============================
Focused optimization on JPM, BAC, WFC which showed 57.6% accuracy.
Target: Push to 70%+ accuracy using hyperparameter optimization.
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import lightgbm as lgb
import talib
import optuna

class OptimizedBankStrategy:
    """Optimized strategy for bank stocks (JPM, BAC, WFC)"""
    
    def __init__(self):
        # Use the symbols that performed best
        self.symbols = ['JPM', 'BAC', 'WFC']
        self.data_period = '2y'  # Use 2 years as enhanced test showed this works
        self.models = {}
        self.scalers = {}
        
        print("OPTIMIZED BANK TRADING STRATEGY")
        print("=" * 50)
        print(f"Target Symbols: {', '.join(self.symbols)}")
        print(f"Previous Best: 57.6% accuracy")
        print(f"Target: 70%+ accuracy")
        print("=" * 50)
    
    def get_bank_data(self):
        """Get enhanced bank data"""
        
        print("FETCHING BANK DATA...")
        
        try:
            # Get 2 years of bank data
            data = yf.download(self.symbols, period=self.data_period, progress=False)
            print(f"   SUCCESS: {len(data)} days of data")
            
            # Use first symbol if multiple
            if isinstance(data.columns, pd.MultiIndex):
                symbol = self.symbols[0]  # Use JPM
                data = data.xs(symbol, level=1, axis=1)
                print(f"   Using data for: {symbol}")
            
            return data
            
        except Exception as e:
            print(f"   ERROR: {e}")
            # Fallback to sample data
            dates = pd.date_range('2022-01-01', '2024-01-01', freq='D')
            return pd.DataFrame({
                'Open': np.random.randn(len(dates)).cumsum() + 140,
                'High': np.random.randn(len(dates)).cumsum() + 142,
                'Low': np.random.randn(len(dates)).cumsum() + 138,
                'Close': np.random.randn(len(dates)).cumsum() + 140,
                'Volume': np.random.randint(10000000, 50000000, len(dates))
            }, index=dates)
    
    def create_bank_specific_features(self, data):
        """Create features specifically optimized for banks"""
        
        print("CREATING BANK-SPECIFIC FEATURES...")
        
        features = pd.DataFrame(index=data.index)
        close = data['Close'].values.astype(float)
        high = data['High'].values.astype(float)
        low = data['Low'].values.astype(float)
        volume = data['Volume'].values.astype(float)
        
        # Bank-specific momentum features (banks are cyclical)
        print("   Adding momentum features...")
        for period in [3, 5, 10, 15, 20, 30]:
            features[f'MOMENTUM_{period}'] = pd.Series(close).pct_change(period)
            features[f'RSI_{period}'] = talib.RSI(close, timeperiod=period)
        
        # Interest rate sensitivity features (key for banks)
        print("   Adding rate sensitivity features...")
        features['MACD'], features['MACD_SIGNAL'], features['MACD_HIST'] = talib.MACD(close)
        features['ADX'] = talib.ADX(high, low, close)
        features['CCI'] = talib.CCI(high, low, close)
        features['WILLR'] = talib.WILLR(high, low, close)
        
        # Volatility features (banks hate uncertainty)
        print("   Adding volatility features...")
        features['BBANDS_UPPER'], features['BBANDS_MIDDLE'], features['BBANDS_LOWER'] = talib.BBANDS(close)
        features['ATR'] = talib.ATR(high, low, close)
        features['NATR'] = talib.NATR(high, low, close)
        
        # Volume features (institutional flows matter for banks)
        print("   Adding volume features...")
        features['OBV'] = talib.OBV(close, volume)
        features['AD'] = talib.AD(high, low, close, volume)
        features['ADOSC'] = talib.ADOSC(high, low, close, volume)
        
        # Moving averages (trend following for banks)
        print("   Adding moving averages...")
        for period in [5, 10, 15, 20, 30, 50, 100, 200]:
            features[f'SMA_{period}'] = talib.SMA(close, timeperiod=period)
            features[f'EMA_{period}'] = talib.EMA(close, timeperiod=period)
            
            # Price position relative to MA
            sma = talib.SMA(close, timeperiod=period)
            features[f'PRICE_VS_SMA_{period}'] = (close - sma) / sma
        
        # Statistical features
        print("   Adding statistical features...")
        returns = pd.Series(close).pct_change()
        for window in [5, 10, 15, 20, 30]:
            features[f'VOLATILITY_{window}'] = returns.rolling(window).std()
            features[f'SKEWNESS_{window}'] = returns.rolling(window).skew()
            features[f'KURTOSIS_{window}'] = returns.rolling(window).kurt()
        
        # Support/resistance levels
        print("   Adding support/resistance...")
        for period in [10, 20, 30, 50]:
            features[f'SUPPORT_{period}'] = pd.Series(low).rolling(period).min()
            features[f'RESISTANCE_{period}'] = pd.Series(high).rolling(period).max()
            features[f'DIST_SUPPORT_{period}'] = (close - features[f'SUPPORT_{period}']) / close
            features[f'DIST_RESISTANCE_{period}'] = (features[f'RESISTANCE_{period}'] - close) / close
        
        # Trend strength
        print("   Adding trend strength...")
        for period in [10, 20, 30]:
            trend = pd.Series(close).rolling(period).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            features[f'TREND_STRENGTH_{period}'] = trend
        
        # Clean features
        features = features.fillna(method='ffill').fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        print(f"   TOTAL FEATURES: {len(features.columns)}")
        return features
    
    def optimize_bank_models(self, X, y):
        """Use Optuna to optimize specifically for bank trading"""
        
        print("OPTIMIZING FOR BANK TRADING...")
        
        def objective(trial):
            # Optimize for bank-specific parameters
            
            # RandomForest optimization
            rf_params = {
                'n_estimators': trial.suggest_int('rf_n_estimators', 100, 500),
                'max_depth': trial.suggest_int('rf_max_depth', 5, 25),
                'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 15),
                'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None])
            }
            
            # XGBoost optimization
            xgb_params = {
                'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 500),
                'max_depth': trial.suggest_int('xgb_max_depth', 3, 12),
                'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('xgb_subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('xgb_reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('xgb_reg_lambda', 0, 10)
            }
            
            # LightGBM optimization
            lgb_params = {
                'n_estimators': trial.suggest_int('lgb_n_estimators', 100, 500),
                'max_depth': trial.suggest_int('lgb_max_depth', 3, 12),
                'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('lgb_subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('lgb_colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('lgb_reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('lgb_reg_lambda', 0, 10)
            }
            
            # Create models
            rf = RandomForestClassifier(**rf_params, random_state=42, n_jobs=-1)
            xgb_model = xgb.XGBClassifier(**xgb_params, random_state=42)
            lgb_model = lgb.LGBMClassifier(**lgb_params, random_state=42, verbose=-1)
            
            # Use time series cross-validation for banks
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # Scale features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                
                # Train ensemble with voting weights
                rf_weight = trial.suggest_float('rf_weight', 0.1, 1.0)
                xgb_weight = trial.suggest_float('xgb_weight', 0.1, 1.0) 
                lgb_weight = trial.suggest_float('lgb_weight', 0.1, 1.0)
                
                ensemble = VotingClassifier([
                    ('rf', rf),
                    ('xgb', xgb_model),
                    ('lgb', lgb_model)
                ], voting='soft', weights=[rf_weight, xgb_weight, lgb_weight])
                
                ensemble.fit(X_train_scaled, y_train)
                score = ensemble.score(X_val_scaled, y_val)
                scores.append(score)
            
            return np.mean(scores)
        
        # Run optimization
        print("   Running optimization (50 trials)...")
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50, show_progress_bar=False)
        
        print(f"   BEST ACCURACY: {study.best_value:.1%}")
        print(f"   Optimization complete!")
        
        return study.best_params, study.best_value
    
    def train_optimized_ensemble(self, X, y, best_params):
        """Train final ensemble with optimized parameters"""
        
        print("TRAINING OPTIMIZED ENSEMBLE...")
        print(f"   Training samples: {len(X)}")
        print(f"   Features: {X.shape[1]}")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Extract optimized parameters
        rf_params = {k.replace('rf_', ''): v for k, v in best_params.items() if k.startswith('rf_')}
        xgb_params = {k.replace('xgb_', ''): v for k, v in best_params.items() if k.startswith('xgb_')}
        lgb_params = {k.replace('lgb_', ''): v for k, v in best_params.items() if k.startswith('lgb_')}
        
        # Create optimized models
        rf = RandomForestClassifier(**rf_params, random_state=42, n_jobs=-1)
        xgb_model = xgb.XGBClassifier(**xgb_params, random_state=42)
        lgb_model = lgb.LGBMClassifier(**lgb_params, random_state=42, verbose=-1)
        
        # Train individual models
        models = {'RandomForest': rf, 'XGBoost': xgb_model, 'LightGBM': lgb_model}
        results = {}
        
        for name, model in models.items():
            print(f"   Training {name}...")
            model.fit(X_train_scaled, y_train)
            
            train_score = model.score(X_train_scaled, y_train)
            test_score = model.score(X_test_scaled, y_test)
            
            results[name] = {
                'train_accuracy': train_score,
                'test_accuracy': test_score
            }
            
            print(f"      Train: {train_score:.1%}, Test: {test_score:.1%}")
        
        # Create optimized ensemble
        print("   Creating optimized ensemble...")
        weights = [
            best_params.get('rf_weight', 1.0),
            best_params.get('xgb_weight', 1.0),
            best_params.get('lgb_weight', 1.0)
        ]
        
        ensemble = VotingClassifier([
            ('rf', rf),
            ('xgb', xgb_model),
            ('lgb', lgb_model)
        ], voting='soft', weights=weights)
        
        ensemble.fit(X_train_scaled, y_train)
        
        ensemble_train = ensemble.score(X_train_scaled, y_train)
        ensemble_test = ensemble.score(X_test_scaled, y_test)
        
        results['Optimized_Ensemble'] = {
            'train_accuracy': ensemble_train,
            'test_accuracy': ensemble_test
        }
        
        print(f"   OPTIMIZED ENSEMBLE: Train {ensemble_train:.1%}, Test {ensemble_test:.1%}")
        
        # Store models
        self.models['ensemble'] = ensemble
        self.scalers['standard'] = scaler
        
        return results
    
    def run_bank_optimization(self):
        """Run complete bank optimization"""
        
        # Get bank data
        data = self.get_bank_data()
        
        # Create bank-specific features
        features = self.create_bank_specific_features(data)
        
        # Create target
        returns = data['Close'].pct_change()
        target = (returns.shift(-1) > 0).astype(int)
        
        # Align data
        X = features.iloc[:-1]
        y = target.iloc[:-1].dropna()
        X = X.loc[y.index]
        
        print(f"\nDATA SUMMARY:")
        print(f"   Total samples: {len(X)}")
        print(f"   Total features: {X.shape[1]}")
        print(f"   Target distribution: {y.value_counts().to_dict()}")
        
        if len(X) < 200:
            print("   WARNING: Need more data for reliable optimization")
            return None
        
        # Optimize models
        best_params, best_score = self.optimize_bank_models(X, y)
        
        # Train final ensemble
        results = self.train_optimized_ensemble(X, y, best_params)
        
        # Analyze results
        print(f"\nOPTIMIZATION RESULTS:")
        print("=" * 30)
        
        best_model = max(results.keys(), key=lambda k: results[k]['test_accuracy'])
        best_accuracy = results[best_model]['test_accuracy']
        
        print(f"BEST MODEL: {best_model}")
        print(f"BEST ACCURACY: {best_accuracy:.1%}")
        
        if best_accuracy >= 0.70:
            print("ðŸŽ¯ TARGET ACHIEVED! 70%+ accuracy reached!")
        elif best_accuracy >= 0.65:
            print("ðŸ“ˆ EXCELLENT! Close to target - minor tweaks needed")
        elif best_accuracy >= 0.60:
            print("ðŸ’ª GOOD! Significant improvement from 57.6%")
        else:
            print("ðŸ”§ MORE OPTIMIZATION NEEDED")
        
        return results

if __name__ == "__main__":
    print("Starting Bank Strategy Optimization...")
    
    strategy = OptimizedBankStrategy()
    results = strategy.run_bank_optimization()
    
    if results:
        print(f"\nBANK OPTIMIZATION COMPLETE!")
        print(f"Ready for next phase of intensive training.")
    else:
        print(f"\nNeed to address data issues first.")